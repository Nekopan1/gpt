seed: 42

paths:
  dataset_path: data/dataset.json
  tokenizer_dir: tokenizer
  output_dir: models/base

model:
  vocab_size: 32000
  max_position_embeddings: 2048
  n_embd: 2048
  n_layer: 24
  n_head: 16
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1

training:
  max_seq_len: 2048
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 3
  learning_rate: 3e-4
  weight_decay: 0.1
  warmup_steps: 200
  logging_steps: 25
  save_steps: 500
  fp16: true
  gradient_checkpointing: true

chat:
  system_prompt: "You are a helpful, offline assistant with persistent memory."
  max_new_tokens: 256
